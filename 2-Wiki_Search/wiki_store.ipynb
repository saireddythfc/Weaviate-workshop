{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading environment variables\n",
    "\n",
    "import os\n",
    "\n",
    "WEAVIATE_CLUSTER_URL = os.getenv(\"WEAVIATE_CLUSTER_URL\")\n",
    "WEAVIATE_API_KEY = os.getenv(\"WEAVIATE_API_KEY\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Weaviate Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate, os\n",
    "\n",
    "# Connect to a cloud instance of Weaviate (with WCS)\n",
    "client = weaviate.connect_to_wcs(\n",
    "    cluster_url=WEAVIATE_CLUSTER_URL,\n",
    "    auth_credentials=weaviate.auth.AuthApiKey(WEAVIATE_API_KEY),\n",
    ")\n",
    "\n",
    "client.is_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Vector Embeddings for Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weaviate.collections.collection.Collection at 0x11f6fe310>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from weaviate.classes.config import Configure, Property, DataType\n",
    "\n",
    "if client.collections.exists(\"Wikipedia\"):\n",
    "    client.collections.delete(\"Wikipedia\")\n",
    "\n",
    "# Create a collection here - with Cohere as a vectorizer\n",
    "client.collections.create(\n",
    "    name=\"Wikipedia\",\n",
    "    \n",
    "    vectorizer_config=Configure.Vectorizer.text2vec_cohere(\n",
    "        model=\"embed-multilingual-v2.0\"\n",
    "    ),\n",
    "\n",
    "    generative_config=Configure.Generative.openai(\"gpt-4\"),\n",
    "\n",
    "    properties=[\n",
    "        Property(name=\"text\", data_type=DataType.TEXT),\n",
    "        Property(name=\"title\", data_type=DataType.TEXT, skip_vectorization=True),\n",
    "        Property(name=\"wiki_id\", data_type=DataType.INT, skip_vectorization=True),\n",
    "        Property(name=\"url\", data_type=DataType.TEXT, skip_vectorization=True),\n",
    "        Property(name=\"lang\", data_type=DataType.TEXT, skip_vectorization=True),\n",
    "        Property(name=\"lang_id\", data_type=DataType.INT, skip_vectorization=True),\n",
    "        Property(name=\"views\", data_type=DataType.NUMBER, skip_vectorization=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "def import_wiki_data(lang, lang_id, max_rows, skip_rows=0):\n",
    "    print(f\"Importing {max_rows} data items for {lang}\")\n",
    "\n",
    "    dataset = load_dataset(f\"Cohere/wikipedia-22-12-{lang}-embeddings\", split=\"train\", streaming=True)\n",
    "    dataset = dataset.skip(skip_rows)\n",
    "\n",
    "    # counter = 0\n",
    "    counter = skip_rows\n",
    "\n",
    "    wikipedia = client.collections.get(\"Wikipedia\")\n",
    "\n",
    "    with wikipedia.batch.fixed_size(batch_size=1000, concurrent_requests=4) as batch:\n",
    "        for item in tqdm(dataset, initial=skip_rows, total=max_rows):\n",
    "            vector = item[\"emb\"]\n",
    "            data_to_insert = {   \n",
    "                \"text\": item[\"text\"],\n",
    "                \"wiki_id\": item[\"wiki_id\"],\n",
    "                \"title\": item[\"title\"],\n",
    "                \"url\": item[\"url\"],\n",
    "                \"views\": item[\"views\"],\n",
    "                \"lang\": lang,\n",
    "                \"lang_id\": lang_id,\n",
    "            }\n",
    "\n",
    "            batch.add_object(\n",
    "                properties=data_to_insert,\n",
    "                vector=vector\n",
    "            )\n",
    "            \n",
    "            # stop after the request number reaches = max_rows\n",
    "            counter += 1\n",
    "            if counter >= max_rows:\n",
    "                break\n",
    "    \n",
    "    # check for errors at the end\n",
    "    if (len(wikipedia.batch.failed_objects)>0):\n",
    "        print(\"Final error check\")\n",
    "        print(f\"Some errors {len(wikipedia.batch.failed_objects)}\")\n",
    "        print(wikipedia.batch.failed_objects[-1])\n",
    "    \n",
    "    print(f\"Imported {counter} items for {lang}\")\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing 10000 data items for en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 23b6209b-db81-4b06-a0c9-aa173a4210ed)')' thrown while requesting GET https://huggingface.co/datasets/Cohere/wikipedia-22-12-en-embeddings/resolve/85c2eca83d4b9dcecc043c23748cb8c1047f683f/data/train-00000-of-00253-8d3dffb4e6ef0304.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "100%|█████████▉| 9999/10000 [00:54<00:00, 182.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 10000 items for en\n",
      "-----------------------------------\n",
      "Importing 10000 data items for de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 9999/10000 [00:45<00:00, 221.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 10000 items for de\n",
      "-----------------------------------\n",
      "Importing 10000 data items for fr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 9999/10000 [01:11<00:00, 139.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 10000 items for fr\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import_per_country = 10_000\n",
    "\n",
    "import_wiki_data(\"en\", 0, import_per_country, 0)\n",
    "import_wiki_data(\"de\", 1, import_per_country, 0)\n",
    "import_wiki_data(\"fr\", 2, import_per_country, 0)\n",
    "# import_wiki_data(\"es\", 3, import_per_country, 0)\n",
    "# import_wiki_data(\"it\", 4, import_per_country, 0)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
